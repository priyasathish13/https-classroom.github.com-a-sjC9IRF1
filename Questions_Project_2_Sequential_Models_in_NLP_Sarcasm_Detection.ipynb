{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 2 - Sequential Models in NLP - Sarcasm Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7SIeJ_oZMU"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1tpOCamr9aWz817atPnyXus8w5gJ3mIts\" width=500px>\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eukag7wEoPZu"
      },
      "source": [
        "### Package Version:\n",
        "- tensorflow==2.2.0\n",
        "- pandas==1.0.5\n",
        "- numpy==1.18.5\n",
        "- google==2.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp68FAQf9aMN"
      },
      "source": [
        "# Sarcasm Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEahVPtWX5ve"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "#### Acknowledgement\n",
        "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
        "\n",
        "**Required Files given in below link.**\n",
        "\n",
        "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAk6BRUh8CqL"
      },
      "source": [
        "### Load Data (3 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMgrqZJYwz3b",
        "outputId": "e479da63-5fea-4275-e7a3-b66436530f23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow 2.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement 2.3.0 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for 2.3.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzE0FXdVpPMB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu9PKtUyZZCr"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name = \"Sarcasm_Headlines_Dataset.json.zip\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9szo_vMZlap",
        "outputId": "cb7450de-3472-4ffc-81a0-c95e735f536b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "Sarcasm_Headlines_Dataset.json                 2019-10-01 11:35:02      5616830\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Jgi92_aGHy",
        "outputId": "68f6973c-dade-46de-e3d0-40567d9d7264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_json(\"/content/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6pXf7A78E2H"
      },
      "source": [
        "### Drop `article_link` from dataset (3 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUNHq5zEV0n"
      },
      "source": [
        "del df['article_link']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3-dLsv9aVnw",
        "outputId": "67861ba7-35ac-4b36-8f99-b0da19b83dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  is_sarcastic\n",
              "0  former versace store clerk sues over secret 'b...             0\n",
              "1  the 'roseanne' revival catches up to our thorn...             0\n",
              "2  mom starting to fear son's web series closest ...             1\n",
              "3  boehner just wants wife to listen, not come up...             1\n",
              "4  j.k. rowling wishes snape happy birthday in th...             0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0h6IOxU8OdH"
      },
      "source": [
        "### Get length of each headline and add a column for that (3 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLpiBRDmEV2l"
      },
      "source": [
        "text_len1=df[df['is_sarcastic']==0]['headline'].str.len()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMF-wjJ2aMwm"
      },
      "source": [
        "### Initialize parameter values\n",
        "- Set values for max_features, maxlen, & embedding_size\n",
        "- max_features: Number of words to take from tokenizer(most frequent words)\n",
        "- maxlen: Maximum length of each sentence to be limited to 25\n",
        "- embedding_size: size of embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPw9gAN_EV6m"
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 25\n",
        "embedding_size = 200"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abSe-bM8fn9"
      },
      "source": [
        "### Apply `tensorflow.keras` Tokenizer and get indices for words (3 Marks)\n",
        "- Initialize Tokenizer object with number of words as 10000\n",
        "- Fit the tokenizer object on headline column\n",
        "- Convert the text to sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8g4l0KfF3eh"
      },
      "source": [
        "import gensim\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jIiWNqItCHh",
        "outputId": "1b5dfb4e-914c-4e44-8b3b-9892fc682e80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words = []\n",
        "for i in df.headline.values:\n",
        "    words.append(i.split())\n",
        "words[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['former',\n",
              "  'versace',\n",
              "  'store',\n",
              "  'clerk',\n",
              "  'sues',\n",
              "  'over',\n",
              "  'secret',\n",
              "  \"'black\",\n",
              "  \"code'\",\n",
              "  'for',\n",
              "  'minority',\n",
              "  'shoppers'],\n",
              " ['the',\n",
              "  \"'roseanne'\",\n",
              "  'revival',\n",
              "  'catches',\n",
              "  'up',\n",
              "  'to',\n",
              "  'our',\n",
              "  'thorny',\n",
              "  'political',\n",
              "  'mood,',\n",
              "  'for',\n",
              "  'better',\n",
              "  'and',\n",
              "  'worse'],\n",
              " ['mom',\n",
              "  'starting',\n",
              "  'to',\n",
              "  'fear',\n",
              "  \"son's\",\n",
              "  'web',\n",
              "  'series',\n",
              "  'closest',\n",
              "  'thing',\n",
              "  'she',\n",
              "  'will',\n",
              "  'have',\n",
              "  'to',\n",
              "  'grandchild'],\n",
              " ['boehner',\n",
              "  'just',\n",
              "  'wants',\n",
              "  'wife',\n",
              "  'to',\n",
              "  'listen,',\n",
              "  'not',\n",
              "  'come',\n",
              "  'up',\n",
              "  'with',\n",
              "  'alternative',\n",
              "  'debt-reduction',\n",
              "  'ideas'],\n",
              " ['j.k.',\n",
              "  'rowling',\n",
              "  'wishes',\n",
              "  'snape',\n",
              "  'happy',\n",
              "  'birthday',\n",
              "  'in',\n",
              "  'the',\n",
              "  'most',\n",
              "  'magical',\n",
              "  'way']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JtnMgWjsfto"
      },
      "source": [
        "w2v_model = gensim.models.Word2Vec(sentences = words , size=EMBEDDING_DIM , window = 5 , min_count = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQdoBZi3smf2",
        "outputId": "c306e59a-a155-4689-b8d8-d854c06292a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(w2v_model.wv.vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIA8A3aguL9V"
      },
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKuEmeqYTpDy",
        "outputId": "653a7d48-7e53-4e59-8b69-744f53effc0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyb8JgCaTSvb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop =set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsz6NUB_uUsX"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "      if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['headline']=df['headline'].apply(denoise_text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliPSaAoxfD3"
      },
      "source": [
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1YRZw3sroA"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words=35000)\n",
        "tokenizer.fit_on_texts(words)\n",
        "tokenized_train = tokenizer.texts_to_sequences(words)\n",
        "x = sequence.pad_sequences(tokenized_train, maxlen = 20)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeZpwPO4bOkZ"
      },
      "source": [
        "### Pad sequences (3 Marks)\n",
        "- Pad each example with a maximum length\n",
        "- Convert target column into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV0K70E5c9Xl"
      },
      "source": [
        "x = sequence.pad_sequences(tokenized_train, maxlen = 20)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGqzRag2DqAk"
      },
      "source": [
        "def get_weight_matrix(model, vocab):\n",
        "  vocab_size = len(vocab) + 1\n",
        "  weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "  for word, i in vocab.items():\n",
        "    weight_matrix[i] = model[word]\n",
        "    return weight_matrix"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLyKg-98rH_"
      },
      "source": [
        "### Vocab mapping\n",
        "- There is no word for 0th index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCNgtnQqdbZn",
        "outputId": "b15ad159-fdc5-4981-ac88-422c6ac5a6f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 1,\n",
              " 'of': 2,\n",
              " 'the': 3,\n",
              " 'in': 4,\n",
              " 'for': 5,\n",
              " 'a': 6,\n",
              " 'on': 7,\n",
              " 'and': 8,\n",
              " 'with': 9,\n",
              " 'is': 10,\n",
              " 'new': 11,\n",
              " 'man': 12,\n",
              " 'from': 13,\n",
              " 'at': 14,\n",
              " 'trump': 15,\n",
              " 'about': 16,\n",
              " 'you': 17,\n",
              " 'by': 18,\n",
              " 'this': 19,\n",
              " 'after': 20,\n",
              " 'be': 21,\n",
              " 'how': 22,\n",
              " 'out': 23,\n",
              " 'as': 24,\n",
              " 'that': 25,\n",
              " 'up': 26,\n",
              " 'it': 27,\n",
              " 'not': 28,\n",
              " 'your': 29,\n",
              " 'are': 30,\n",
              " 'his': 31,\n",
              " 'what': 32,\n",
              " 'he': 33,\n",
              " 'just': 34,\n",
              " 'who': 35,\n",
              " 'has': 36,\n",
              " 'will': 37,\n",
              " 'more': 38,\n",
              " 'all': 39,\n",
              " 'into': 40,\n",
              " 'have': 41,\n",
              " 'why': 42,\n",
              " 'one': 43,\n",
              " 'area': 44,\n",
              " 'donald': 45,\n",
              " 'over': 46,\n",
              " 'says': 47,\n",
              " 'can': 48,\n",
              " 'woman': 49,\n",
              " 'u.s.': 50,\n",
              " 'day': 51,\n",
              " 'first': 52,\n",
              " \"trump's\": 53,\n",
              " 'her': 54,\n",
              " 'get': 55,\n",
              " 'report:': 56,\n",
              " 'like': 57,\n",
              " 'time': 58,\n",
              " 'an': 59,\n",
              " 'no': 60,\n",
              " 'people': 61,\n",
              " 'now': 62,\n",
              " 'off': 63,\n",
              " 'still': 64,\n",
              " 'obama': 65,\n",
              " 'than': 66,\n",
              " 'was': 67,\n",
              " 'make': 68,\n",
              " 'house': 69,\n",
              " 'life': 70,\n",
              " 'my': 71,\n",
              " 'women': 72,\n",
              " 'white': 73,\n",
              " 'if': 74,\n",
              " 'when': 75,\n",
              " 'i': 76,\n",
              " 'back': 77,\n",
              " 'clinton': 78,\n",
              " 'down': 79,\n",
              " 'could': 80,\n",
              " 'we': 81,\n",
              " 'their': 82,\n",
              " 'before': 83,\n",
              " 'world': 84,\n",
              " 'americans': 85,\n",
              " 'do': 86,\n",
              " 'most': 87,\n",
              " 'family': 88,\n",
              " \"it's\": 89,\n",
              " 'way': 90,\n",
              " 'they': 91,\n",
              " 'gop': 92,\n",
              " 'should': 93,\n",
              " 'would': 94,\n",
              " 'bill': 95,\n",
              " '5': 96,\n",
              " 'best': 97,\n",
              " 'black': 98,\n",
              " 'him': 99,\n",
              " 'police': 100,\n",
              " 'only': 101,\n",
              " 'being': 102,\n",
              " 'really': 103,\n",
              " 'years': 104,\n",
              " 'school': 105,\n",
              " \"can't\": 106,\n",
              " 'but': 107,\n",
              " 'so': 108,\n",
              " 'during': 109,\n",
              " 'finds': 110,\n",
              " 'know': 111,\n",
              " 'death': 112,\n",
              " 'health': 113,\n",
              " 'american': 114,\n",
              " 'last': 115,\n",
              " 'going': 116,\n",
              " 'say': 117,\n",
              " \"'the\": 118,\n",
              " 'home': 119,\n",
              " 'things': 120,\n",
              " 'or': 121,\n",
              " 'nation': 122,\n",
              " 'against': 123,\n",
              " 'hillary': 124,\n",
              " 'may': 125,\n",
              " 'every': 126,\n",
              " 'show': 127,\n",
              " 'she': 128,\n",
              " 'state': 129,\n",
              " 'campaign': 130,\n",
              " 'president': 131,\n",
              " 'year': 132,\n",
              " 'need': 133,\n",
              " 'mom': 134,\n",
              " 'good': 135,\n",
              " 'gets': 136,\n",
              " 'big': 137,\n",
              " 'getting': 138,\n",
              " 'these': 139,\n",
              " 'too': 140,\n",
              " 'watch': 141,\n",
              " 'some': 142,\n",
              " 'love': 143,\n",
              " 'little': 144,\n",
              " 'take': 145,\n",
              " 'video': 146,\n",
              " 'parents': 147,\n",
              " 'makes': 148,\n",
              " 'our': 149,\n",
              " 'kids': 150,\n",
              " 'calls': 151,\n",
              " 'party': 152,\n",
              " 'john': 153,\n",
              " '10': 154,\n",
              " \"doesn't\": 155,\n",
              " 'through': 156,\n",
              " 'while': 157,\n",
              " \"here's\": 158,\n",
              " 'other': 159,\n",
              " 'work': 160,\n",
              " 'court': 161,\n",
              " 'want': 162,\n",
              " 'own': 163,\n",
              " 'right': 164,\n",
              " \"don't\": 165,\n",
              " 'study': 166,\n",
              " '3': 167,\n",
              " 'news': 168,\n",
              " 'takes': 169,\n",
              " 'never': 170,\n",
              " 'change': 171,\n",
              " 'child': 172,\n",
              " 'local': 173,\n",
              " 'where': 174,\n",
              " 'see': 175,\n",
              " 'its': 176,\n",
              " '--': 177,\n",
              " 'even': 178,\n",
              " \"he's\": 179,\n",
              " 'stop': 180,\n",
              " \"nation's\": 181,\n",
              " 'look': 182,\n",
              " 'next': 183,\n",
              " 'college': 184,\n",
              " 'gay': 185,\n",
              " 'another': 186,\n",
              " 'war': 187,\n",
              " 'guy': 188,\n",
              " 'plan': 189,\n",
              " 'election': 190,\n",
              " 'dead': 191,\n",
              " 'made': 192,\n",
              " 'got': 193,\n",
              " 'bush': 194,\n",
              " 'america': 195,\n",
              " 'around': 196,\n",
              " 'wants': 197,\n",
              " 'dad': 198,\n",
              " 'go': 199,\n",
              " 'office': 200,\n",
              " 'real': 201,\n",
              " 'thing': 202,\n",
              " 'million': 203,\n",
              " 'dog': 204,\n",
              " 'gun': 205,\n",
              " 'report': 206,\n",
              " 'north': 207,\n",
              " 'been': 208,\n",
              " 'help': 209,\n",
              " 'finally': 210,\n",
              " 'congress': 211,\n",
              " 'us': 212,\n",
              " 'them': 213,\n",
              " 'job': 214,\n",
              " 'care': 215,\n",
              " 'again': 216,\n",
              " 'under': 217,\n",
              " \"man's\": 218,\n",
              " 'high': 219,\n",
              " 'national': 220,\n",
              " 'much': 221,\n",
              " 'debate': 222,\n",
              " 'baby': 223,\n",
              " 'week': 224,\n",
              " 'couple': 225,\n",
              " \"won't\": 226,\n",
              " 'actually': 227,\n",
              " 'two': 228,\n",
              " 'ever': 229,\n",
              " 'climate': 230,\n",
              " 'shows': 231,\n",
              " 'give': 232,\n",
              " 'had': 233,\n",
              " 'ways': 234,\n",
              " 'sex': 235,\n",
              " 'money': 236,\n",
              " '7': 237,\n",
              " 'teen': 238,\n",
              " 'reveals': 239,\n",
              " 'top': 240,\n",
              " 'star': 241,\n",
              " 'trying': 242,\n",
              " 'sexual': 243,\n",
              " 'season': 244,\n",
              " 'supreme': 245,\n",
              " 'night': 246,\n",
              " 'announces': 247,\n",
              " 'paul': 248,\n",
              " '6': 249,\n",
              " 'shooting': 250,\n",
              " 'without': 251,\n",
              " '2': 252,\n",
              " 'old': 253,\n",
              " 'senate': 254,\n",
              " 'better': 255,\n",
              " 'media': 256,\n",
              " 'students': 257,\n",
              " 'there': 258,\n",
              " 'food': 259,\n",
              " 'everyone': 260,\n",
              " 'history': 261,\n",
              " '4': 262,\n",
              " 'any': 263,\n",
              " 'fight': 264,\n",
              " 'introduces': 265,\n",
              " 'making': 266,\n",
              " 'attack': 267,\n",
              " 'does': 268,\n",
              " 'away': 269,\n",
              " 'end': 270,\n",
              " 'former': 271,\n",
              " 'facebook': 272,\n",
              " 'entire': 273,\n",
              " 'enough': 274,\n",
              " 'pope': 275,\n",
              " '&': 276,\n",
              " 'call': 277,\n",
              " 'face': 278,\n",
              " 'film': 279,\n",
              " 'movie': 280,\n",
              " 'son': 281,\n",
              " 'tell': 282,\n",
              " 'tv': 283,\n",
              " 'come': 284,\n",
              " 'found': 285,\n",
              " 'god': 286,\n",
              " 'york': 287,\n",
              " 'business': 288,\n",
              " 'game': 289,\n",
              " 'find': 290,\n",
              " 'friend': 291,\n",
              " 'children': 292,\n",
              " 'bad': 293,\n",
              " 'government': 294,\n",
              " 'think': 295,\n",
              " 'city': 296,\n",
              " 'part': 297,\n",
              " 'men': 298,\n",
              " 'law': 299,\n",
              " 'great': 300,\n",
              " 'live': 301,\n",
              " 'self': 302,\n",
              " 'story': 303,\n",
              " 'deal': 304,\n",
              " 'republican': 305,\n",
              " 'fire': 306,\n",
              " 'support': 307,\n",
              " 'body': 308,\n",
              " 'thinks': 309,\n",
              " 'presidential': 310,\n",
              " 'use': 311,\n",
              " 'keep': 312,\n",
              " 'wedding': 313,\n",
              " 'morning': 314,\n",
              " 'me': 315,\n",
              " 'friends': 316,\n",
              " 'public': 317,\n",
              " 'pretty': 318,\n",
              " 'james': 319,\n",
              " 'photos': 320,\n",
              " 'speech': 321,\n",
              " 'behind': 322,\n",
              " 'single': 323,\n",
              " 'name': 324,\n",
              " 'already': 325,\n",
              " 'between': 326,\n",
              " 'releases': 327,\n",
              " 'company': 328,\n",
              " \"didn't\": 329,\n",
              " 'car': 330,\n",
              " 'book': 331,\n",
              " 'sanders': 332,\n",
              " 'republicans': 333,\n",
              " \"world's\": 334,\n",
              " 'tax': 335,\n",
              " 'rights': 336,\n",
              " 'power': 337,\n",
              " 'human': 338,\n",
              " 'might': 339,\n",
              " 'violence': 340,\n",
              " 'talk': 341,\n",
              " 'case': 342,\n",
              " 'must': 343,\n",
              " 'used': 344,\n",
              " 'girl': 345,\n",
              " 'coming': 346,\n",
              " 'marriage': 347,\n",
              " 'security': 348,\n",
              " 'goes': 349,\n",
              " 'democrats': 350,\n",
              " 'killed': 351,\n",
              " 'doing': 352,\n",
              " 'group': 353,\n",
              " 'having': 354,\n",
              " 'line': 355,\n",
              " '8': 356,\n",
              " 'student': 357,\n",
              " 'secret': 358,\n",
              " 'voters': 359,\n",
              " 'asks': 360,\n",
              " 'vote': 361,\n",
              " 'bernie': 362,\n",
              " 'christmas': 363,\n",
              " 'open': 364,\n",
              " 'run': 365,\n",
              " 'study:': 366,\n",
              " 'each': 367,\n",
              " 'twitter': 368,\n",
              " 'something': 369,\n",
              " 'room': 370,\n",
              " 'because': 371,\n",
              " 'looking': 372,\n",
              " 'win': 373,\n",
              " 'once': 374,\n",
              " 'ban': 375,\n",
              " 'future': 376,\n",
              " 'department': 377,\n",
              " 'ad': 378,\n",
              " 'team': 379,\n",
              " 'claims': 380,\n",
              " 'forced': 381,\n",
              " 'fans': 382,\n",
              " 'teacher': 383,\n",
              " 'unveils': 384,\n",
              " 'sure': 385,\n",
              " 'meet': 386,\n",
              " 'missing': 387,\n",
              " '2016': 388,\n",
              " 'free': 389,\n",
              " 'judge': 390,\n",
              " 'running': 391,\n",
              " 'middle': 392,\n",
              " 'ryan': 393,\n",
              " 'many': 394,\n",
              " 'political': 395,\n",
              " 'inside': 396,\n",
              " 'reports': 397,\n",
              " 'music': 398,\n",
              " 'tells': 399,\n",
              " 'scientists': 400,\n",
              " 'second': 401,\n",
              " 'social': 402,\n",
              " 'long': 403,\n",
              " 'put': 404,\n",
              " 'perfect': 405,\n",
              " 'save': 406,\n",
              " 'always': 407,\n",
              " 'until': 408,\n",
              " 'plans': 409,\n",
              " 'very': 410,\n",
              " 'photo': 411,\n",
              " 'were': 412,\n",
              " 'super': 413,\n",
              " 'female': 414,\n",
              " 'same': 415,\n",
              " 'ready': 416,\n",
              " 'red': 417,\n",
              " 'control': 418,\n",
              " 'dies': 419,\n",
              " 'michael': 420,\n",
              " 'looks': 421,\n",
              " 'employee': 422,\n",
              " 'minutes': 423,\n",
              " 'race': 424,\n",
              " 'days': 425,\n",
              " 'art': 426,\n",
              " 'did': 427,\n",
              " 'wife': 428,\n",
              " 'water': 429,\n",
              " 'living': 430,\n",
              " 'comes': 431,\n",
              " 'boy': 432,\n",
              " 'california': 433,\n",
              " 'obamacare': 434,\n",
              " 'talks': 435,\n",
              " 'needs': 436,\n",
              " 'country': 437,\n",
              " 'past': 438,\n",
              " 'set': 439,\n",
              " 'taking': 440,\n",
              " 'gives': 441,\n",
              " 'candidate': 442,\n",
              " 'let': 443,\n",
              " 'warns': 444,\n",
              " 'summer': 445,\n",
              " 'everything': 446,\n",
              " 'head': 447,\n",
              " \"women's\": 448,\n",
              " 'admits': 449,\n",
              " 'mike': 450,\n",
              " 'person': 451,\n",
              " 'ceo': 452,\n",
              " 'pay': 453,\n",
              " 'list': 454,\n",
              " \"'i\": 455,\n",
              " 'father': 456,\n",
              " \"you're\": 457,\n",
              " 'justice': 458,\n",
              " 'george': 459,\n",
              " 'left': 460,\n",
              " 'idea': 461,\n",
              " 'texas': 462,\n",
              " 'kim': 463,\n",
              " \"i'm\": 464,\n",
              " 'letter': 465,\n",
              " '20': 466,\n",
              " 'secretary': 467,\n",
              " 'start': 468,\n",
              " 'record': 469,\n",
              " 'meeting': 470,\n",
              " 'times': 471,\n",
              " 'wins': 472,\n",
              " 'hours': 473,\n",
              " 'thought': 474,\n",
              " 'lives': 475,\n",
              " 'south': 476,\n",
              " 'working': 477,\n",
              " 'percent': 478,\n",
              " 'full': 479,\n",
              " 'probably': 480,\n",
              " 'mother': 481,\n",
              " 'town': 482,\n",
              " 'here': 483,\n",
              " 'three': 484,\n",
              " 'shot': 485,\n",
              " 'thousands': 486,\n",
              " 'hot': 487,\n",
              " 'biden': 488,\n",
              " 'someone': 489,\n",
              " 'cancer': 490,\n",
              " 'fucking': 491,\n",
              " 'age': 492,\n",
              " \"she's\": 493,\n",
              " 'wrong': 494,\n",
              " '12': 495,\n",
              " 'tips': 496,\n",
              " 'few': 497,\n",
              " 'service': 498,\n",
              " 'states': 499,\n",
              " 'young': 500,\n",
              " 'fbi': 501,\n",
              " 'leaves': 502,\n",
              " 'ted': 503,\n",
              " 'lost': 504,\n",
              " 'dream': 505,\n",
              " 'questions': 506,\n",
              " 'restaurant': 507,\n",
              " 'wall': 508,\n",
              " 'daughter': 509,\n",
              " 'believe': 510,\n",
              " 'feel': 511,\n",
              " 'federal': 512,\n",
              " 'chris': 513,\n",
              " 'cruz': 514,\n",
              " 'russia': 515,\n",
              " 'guide': 516,\n",
              " 'administration': 517,\n",
              " 'tweets': 518,\n",
              " 'talking': 519,\n",
              " 'cat': 520,\n",
              " 'eating': 521,\n",
              " 'fan': 522,\n",
              " \"what's\": 523,\n",
              " 'officials': 524,\n",
              " 'today': 525,\n",
              " 'owner': 526,\n",
              " 'nothing': 527,\n",
              " 'together': 528,\n",
              " 'online': 529,\n",
              " 'street': 530,\n",
              " 'latest': 531,\n",
              " 'korea': 532,\n",
              " 'nuclear': 533,\n",
              " 'giving': 534,\n",
              " 'military': 535,\n",
              " 'place': 536,\n",
              " 'yet': 537,\n",
              " 'florida': 538,\n",
              " 'internet': 539,\n",
              " 'buy': 540,\n",
              " 'small': 541,\n",
              " 'months': 542,\n",
              " 'majority': 543,\n",
              " 'heart': 544,\n",
              " 'democratic': 545,\n",
              " 'drug': 546,\n",
              " 'phone': 547,\n",
              " 'class': 548,\n",
              " 'march': 549,\n",
              " 'kill': 550,\n",
              " 'education': 551,\n",
              " 'romney': 552,\n",
              " '-': 553,\n",
              " 'director': 554,\n",
              " 'told': 555,\n",
              " 'outside': 556,\n",
              " 'different': 557,\n",
              " 'move': 558,\n",
              " \"isn't\": 559,\n",
              " 'reason': 560,\n",
              " 'holiday': 561,\n",
              " 'chief': 562,\n",
              " 'prison': 563,\n",
              " 'lot': 564,\n",
              " 'those': 565,\n",
              " 'favorite': 566,\n",
              " 'isis': 567,\n",
              " 'crisis': 568,\n",
              " 'word': 569,\n",
              " 'response': 570,\n",
              " 'ask': 571,\n",
              " 'reasons': 572,\n",
              " 'attacks': 573,\n",
              " 'rock': 574,\n",
              " 'series': 575,\n",
              " 'birthday': 576,\n",
              " \"america's\": 577,\n",
              " 'congressman': 578,\n",
              " 'francis': 579,\n",
              " 'air': 580,\n",
              " 'fox': 581,\n",
              " 'celebrates': 582,\n",
              " 'hollywood': 583,\n",
              " 'issues': 584,\n",
              " 'beautiful': 585,\n",
              " 'personal': 586,\n",
              " 'huffpost': 587,\n",
              " 'less': 588,\n",
              " 'email:': 589,\n",
              " 'protest': 590,\n",
              " 'bar': 591,\n",
              " 'earth': 592,\n",
              " 'following': 593,\n",
              " \"obama's\": 594,\n",
              " 'leave': 595,\n",
              " 'using': 596,\n",
              " 'offers': 597,\n",
              " 'travel': 598,\n",
              " 'mark': 599,\n",
              " 'washington': 600,\n",
              " 'excited': 601,\n",
              " 'rules': 602,\n",
              " 'order': 603,\n",
              " 'king': 604,\n",
              " 'stephen': 605,\n",
              " 'month': 606,\n",
              " 'scott': 607,\n",
              " 'gift': 608,\n",
              " 'play': 609,\n",
              " 'community': 610,\n",
              " 'assault': 611,\n",
              " 'trailer': 612,\n",
              " 'since': 613,\n",
              " 'trip': 614,\n",
              " 'poll': 615,\n",
              " 'knows': 616,\n",
              " 'russian': 617,\n",
              " 'watching': 618,\n",
              " 'sleep': 619,\n",
              " 'visit': 620,\n",
              " 'box': 621,\n",
              " 'lessons': 622,\n",
              " 'david': 623,\n",
              " 'accused': 624,\n",
              " 'special': 625,\n",
              " 'system': 626,\n",
              " 'himself': 627,\n",
              " 'relationship': 628,\n",
              " 'hit': 629,\n",
              " 'stars': 630,\n",
              " 'read': 631,\n",
              " 'iran': 632,\n",
              " 'ice': 633,\n",
              " 'girls': 634,\n",
              " 'hard': 635,\n",
              " 'happy': 636,\n",
              " 'huge': 637,\n",
              " 'muslim': 638,\n",
              " '2015': 639,\n",
              " 'union': 640,\n",
              " 'taylor': 641,\n",
              " 'message': 642,\n",
              " 'kid': 643,\n",
              " 'tom': 644,\n",
              " 'moment': 645,\n",
              " '9': 646,\n",
              " 'jimmy': 647,\n",
              " 'cover': 648,\n",
              " 'leaders': 649,\n",
              " '1': 650,\n",
              " 'hope': 651,\n",
              " 'least': 652,\n",
              " 'fun': 653,\n",
              " 'thinking': 654,\n",
              " 'bring': 655,\n",
              " 'kind': 656,\n",
              " 'become': 657,\n",
              " 'opens': 658,\n",
              " 'whole': 659,\n",
              " 'chinese': 660,\n",
              " 'adorable': 661,\n",
              " 'birth': 662,\n",
              " 'weekend': 663,\n",
              " 'immigration': 664,\n",
              " 'learned': 665,\n",
              " 'victims': 666,\n",
              " 'problem': 667,\n",
              " \"they're\": 668,\n",
              " 'hate': 669,\n",
              " 'date': 670,\n",
              " 'billion': 671,\n",
              " 'conversation': 672,\n",
              " 'third': 673,\n",
              " 'front': 674,\n",
              " 'returns': 675,\n",
              " 'totally': 676,\n",
              " 'hair': 677,\n",
              " 'millions': 678,\n",
              " 'syrian': 679,\n",
              " 'straight': 680,\n",
              " 'powerful': 681,\n",
              " 'cops': 682,\n",
              " 'breaks': 683,\n",
              " 'united': 684,\n",
              " 'leader': 685,\n",
              " 'shit': 686,\n",
              " 'career': 687,\n",
              " 'waiting': 688,\n",
              " 'cop': 689,\n",
              " 'joe': 690,\n",
              " 'china': 691,\n",
              " 'break': 692,\n",
              " 'killing': 693,\n",
              " 'almost': 694,\n",
              " 'fashion': 695,\n",
              " 'turn': 696,\n",
              " 'worried': 697,\n",
              " 'kills': 698,\n",
              " 'reality': 699,\n",
              " 'mass': 700,\n",
              " 'senator': 701,\n",
              " 'fall': 702,\n",
              " 'investigation': 703,\n",
              " 'interview': 704,\n",
              " 'turns': 705,\n",
              " 'puts': 706,\n",
              " 'spends': 707,\n",
              " 'early': 708,\n",
              " 'vows': 709,\n",
              " 'massive': 710,\n",
              " 'queer': 711,\n",
              " 'point': 712,\n",
              " 'abortion': 713,\n",
              " '11': 714,\n",
              " 'anything': 715,\n",
              " 'feels': 716,\n",
              " 'girlfriend': 717,\n",
              " 'schools': 718,\n",
              " 'candidates': 719,\n",
              " 'reportedly': 720,\n",
              " 'hits': 721,\n",
              " 'apple': 722,\n",
              " 'awards': 723,\n",
              " 'adds': 724,\n",
              " 'drunk': 725,\n",
              " 'final': 726,\n",
              " 'global': 727,\n",
              " 'wearing': 728,\n",
              " 'key': 729,\n",
              " 'watch:': 730,\n",
              " 'anniversary': 731,\n",
              " 'policy': 732,\n",
              " 'politics': 733,\n",
              " 'stage': 734,\n",
              " \"there's\": 735,\n",
              " 'called': 736,\n",
              " 'trans': 737,\n",
              " 'crash': 738,\n",
              " 'struggling': 739,\n",
              " 'transgender': 740,\n",
              " 'names': 741,\n",
              " 'song': 742,\n",
              " 'nfl': 743,\n",
              " 'dance': 744,\n",
              " 'across': 745,\n",
              " 'kardashian': 746,\n",
              " 'true': 747,\n",
              " 'moving': 748,\n",
              " 'murder': 749,\n",
              " 'keeps': 750,\n",
              " 'dinner': 751,\n",
              " 'sports': 752,\n",
              " 'worst': 753,\n",
              " 'brings': 754,\n",
              " 'discover': 755,\n",
              " 'return': 756,\n",
              " 'completely': 757,\n",
              " 'signs': 758,\n",
              " 'sign': 759,\n",
              " 'prince': 760,\n",
              " 'center': 761,\n",
              " 'starting': 762,\n",
              " 'decision': 763,\n",
              " 'sick': 764,\n",
              " 'whether': 765,\n",
              " 'longer': 766,\n",
              " 'advice': 767,\n",
              " 'worth': 768,\n",
              " 'hands': 769,\n",
              " 'planned': 770,\n",
              " 'dating': 771,\n",
              " 'hoping': 772,\n",
              " 'steve': 773,\n",
              " 'university': 774,\n",
              " 'host': 775,\n",
              " 'pence': 776,\n",
              " 'given': 777,\n",
              " 'words': 778,\n",
              " 'workers': 779,\n",
              " 'bus': 780,\n",
              " 'trump:': 781,\n",
              " 'rubio': 782,\n",
              " 'weird': 783,\n",
              " '15': 784,\n",
              " 'lead': 785,\n",
              " 'possible': 786,\n",
              " 'test': 787,\n",
              " 'audience': 788,\n",
              " 'apartment': 789,\n",
              " 'important': 790,\n",
              " 'fighting': 791,\n",
              " '9/11': 792,\n",
              " 'seen': 793,\n",
              " 'stand': 794,\n",
              " 'chance': 795,\n",
              " 'band': 796,\n",
              " 'biggest': 797,\n",
              " 'protesters': 798,\n",
              " 'hall': 799,\n",
              " 'store': 800,\n",
              " 'program': 801,\n",
              " 'lose': 802,\n",
              " 'oil': 803,\n",
              " 'nyc': 804,\n",
              " 'which': 805,\n",
              " 'clearly': 806,\n",
              " 'major': 807,\n",
              " 'force': 808,\n",
              " 'michelle': 809,\n",
              " 'road': 810,\n",
              " 'remember': 811,\n",
              " 'chicago': 812,\n",
              " 'employees': 813,\n",
              " '30': 814,\n",
              " 'success': 815,\n",
              " 'playing': 816,\n",
              " 'mental': 817,\n",
              " 'coworker': 818,\n",
              " 'hear': 819,\n",
              " 'die': 820,\n",
              " 'demands': 821,\n",
              " 'artist': 822,\n",
              " 'coffee': 823,\n",
              " 'role': 824,\n",
              " 'celebrate': 825,\n",
              " 'push': 826,\n",
              " 'eat': 827,\n",
              " 'post': 828,\n",
              " 'hurricane': 829,\n",
              " 'oscar': 830,\n",
              " 'five': 831,\n",
              " 'executive': 832,\n",
              " 'sean': 833,\n",
              " 'paris': 834,\n",
              " 'suspect': 835,\n",
              " 'throws': 836,\n",
              " 'ben': 837,\n",
              " 'light': 838,\n",
              " 'tour': 839,\n",
              " 'pregnant': 840,\n",
              " 'industry': 841,\n",
              " 'amazon': 842,\n",
              " '2017': 843,\n",
              " 'experience': 844,\n",
              " 'suicide': 845,\n",
              " 'wishes': 846,\n",
              " 'urges': 847,\n",
              " 'syria': 848,\n",
              " 'simple': 849,\n",
              " 'robert': 850,\n",
              " 'members': 851,\n",
              " 'surprise': 852,\n",
              " '50': 853,\n",
              " 'google': 854,\n",
              " 'risk': 855,\n",
              " 'west': 856,\n",
              " 'allegations': 857,\n",
              " 'receives': 858,\n",
              " 'act': 859,\n",
              " 'halloween': 860,\n",
              " 'homeless': 861,\n",
              " 'uses': 862,\n",
              " 'hand': 863,\n",
              " 'governor': 864,\n",
              " 'fails': 865,\n",
              " 'voter': 866,\n",
              " 'wait': 867,\n",
              " \"we're\": 868,\n",
              " 'try': 869,\n",
              " 'shares': 870,\n",
              " 'dying': 871,\n",
              " 'suggests': 872,\n",
              " 'reform': 873,\n",
              " 'results': 874,\n",
              " 'press': 875,\n",
              " 'official': 876,\n",
              " 'side': 877,\n",
              " 'fear': 878,\n",
              " 'reveal': 879,\n",
              " 'problems': 880,\n",
              " 'abuse': 881,\n",
              " 'check': 882,\n",
              " 'jenner': 883,\n",
              " 'building': 884,\n",
              " 'weight': 885,\n",
              " 'learn': 886,\n",
              " 'number': 887,\n",
              " 'supporters': 888,\n",
              " 'iraq': 889,\n",
              " 'leaving': 890,\n",
              " 'address': 891,\n",
              " 'rest': 892,\n",
              " 'performance': 893,\n",
              " 'album': 894,\n",
              " 'amid': 895,\n",
              " 'begins': 896,\n",
              " 'deadly': 897,\n",
              " 'far': 898,\n",
              " 'poor': 899,\n",
              " 'iowa': 900,\n",
              " 'quietly': 901,\n",
              " 'beauty': 902,\n",
              " 'website': 903,\n",
              " 'cut': 904,\n",
              " 'magazine': 905,\n",
              " 'feeling': 906,\n",
              " 'chicken': 907,\n",
              " 'green': 908,\n",
              " 'elizabeth': 909,\n",
              " 'asking': 910,\n",
              " 'emotional': 911,\n",
              " 'late': 912,\n",
              " 'driving': 913,\n",
              " \"mother's\": 914,\n",
              " 'steps': 915,\n",
              " 'carolina': 916,\n",
              " 'epa': 917,\n",
              " 'voice': 918,\n",
              " 'netflix': 919,\n",
              " 'shop': 920,\n",
              " 'teens': 921,\n",
              " 'plane': 922,\n",
              " 'card': 923,\n",
              " 'football': 924,\n",
              " \"year's\": 925,\n",
              " 'general': 926,\n",
              " 'likely': 927,\n",
              " 'data': 928,\n",
              " 'cnn': 929,\n",
              " 'picture': 930,\n",
              " 'pick': 931,\n",
              " 'explains': 932,\n",
              " 'mind': 933,\n",
              " 'older': 934,\n",
              " 'board': 935,\n",
              " 'lgbt': 936,\n",
              " 'moore': 937,\n",
              " 'leads': 938,\n",
              " 'users': 939,\n",
              " 'hilarious': 940,\n",
              " 'pizza': 941,\n",
              " 'rally': 942,\n",
              " 'lets': 943,\n",
              " 'apparently': 944,\n",
              " \"let's\": 945,\n",
              " 'went': 946,\n",
              " 'ideas': 947,\n",
              " 'toward': 948,\n",
              " 'opening': 949,\n",
              " 'apologizes': 950,\n",
              " 'queen': 951,\n",
              " 'responds': 952,\n",
              " 'documentary': 953,\n",
              " 'near': 954,\n",
              " 'peace': 955,\n",
              " 'voting': 956,\n",
              " 'space': 957,\n",
              " 'prevent': 958,\n",
              " 'private': 959,\n",
              " 'despite': 960,\n",
              " 'perfectly': 961,\n",
              " 'loses': 962,\n",
              " 'oscars': 963,\n",
              " 'planet': 964,\n",
              " 'harassment': 965,\n",
              " 'kerry': 966,\n",
              " 'jobs': 967,\n",
              " 'marijuana': 968,\n",
              " 'commercial': 969,\n",
              " 'leading': 970,\n",
              " 'moms': 971,\n",
              " 'demand': 972,\n",
              " 'arrested': 973,\n",
              " 'gave': 974,\n",
              " 'grandma': 975,\n",
              " 'spend': 976,\n",
              " 'vacation': 977,\n",
              " 'historical': 978,\n",
              " 'question': 979,\n",
              " 'slams': 980,\n",
              " 'olympic': 981,\n",
              " 'residents': 982,\n",
              " 'foreign': 983,\n",
              " 'sales': 984,\n",
              " 'ferguson': 985,\n",
              " 'amazing': 986,\n",
              " 'williams': 987,\n",
              " 'economy': 988,\n",
              " 'eric': 989,\n",
              " 'reminds': 990,\n",
              " '2014': 991,\n",
              " 'driver': 992,\n",
              " 'loss': 993,\n",
              " 'male': 994,\n",
              " 'fake': 995,\n",
              " 'bathroom': 996,\n",
              " \"woman's\": 997,\n",
              " 'bowl': 998,\n",
              " 'spring': 999,\n",
              " 'church': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiu6RWaD92I",
        "outputId": "144ef4d0-acdd-44da-9d21-070d132d44e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRiNX58Rb3oJ"
      },
      "source": [
        "### Set number of words\n",
        "- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfwq6ou8ck2f",
        "outputId": "1745b1cd-30de-4c3a-e447-e0d9672adcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_words = len(tokenizer.word_index) + 1\n",
        "print(num_words)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUF1TuQa8ux0"
      },
      "source": [
        "### Load Glove Word Embeddings (3 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq5AIfRtMeZh"
      },
      "source": [
        "EMBEDDING_FILE = '/content/glove.twitter.27B.200d.txt'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSLNJHdgSTOB"
      },
      "source": [
        "def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHSzdQUcZhm"
      },
      "source": [
        "### Create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elZ-T5aFGZmZ"
      },
      "source": [
        "embeddings = {}\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    word = o.split(\" \")[0]\n",
        "    # print(word)\n",
        "    embd = o.split(\" \")[1:]\n",
        "    embd = np.asarray(embd, dtype='float32')\n",
        "    # print(embd)\n",
        "    embeddings[word] = embd\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IbWuEX82Ra"
      },
      "source": [
        "### Define model (5 Marks)\n",
        "- Hint: Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required. \n",
        "In the end add a final dense layer with sigmoid activation for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv168Gmc3PY"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 2\n",
        "embed_size = 200\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model.add(Embedding(num_words, output_dim=embed_size, weights=[embedding_matrix], input_length=200, trainable=True))\n",
        "#LSTM \n",
        "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.5 , dropout = 0.5)))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoI7_8Y1cqTj"
      },
      "source": [
        "### Compile the model (3 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJiPHeNoJ3U",
        "outputId": "19d7e935-c41e-4e00-f143-f223ad5b0f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s4nmqcecw3a"
      },
      "source": [
        "### Fit the model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN789zNnJ5PL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(df.headline,df.is_sarcastic, test_size = 0.3 , random_state = 0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSHkFgm1R0-W",
        "outputId": "c594e0d6-a1ed-4a21-ad4d-a6d79bb3b148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (x_test,y_test) , epochs = 2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4c86f006772a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: 2 root error(s) found.\n  (0) Unimplemented:  Cast string to float is not supported\n\t [[node sequential_2/Cast (defined at <ipython-input-37-4c86f006772a>:1) ]]\n  (1) Cancelled:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_8584]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12KzHJmRR3j3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}